{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5284ab1-6730-473b-a86d-727bb85e726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84da2c1-a162-4224-8505-cf0a06393696",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we will analyze all of the data in the Disruptions folder. This will be done in the codeblock below using the sklearn module (and regressor).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af3a771e-b896-42ca-9dec-610d64714bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     disruption_count  hour  day  month  year  is_peak  \\\n",
      "start_time                                                               \n",
      "2011-01-10 23:00:00                 0    23    0      1  2011    False   \n",
      "2011-01-11 00:00:00                 0     0    1      1  2011    False   \n",
      "2011-01-11 01:00:00                 0     1    1      1  2011    False   \n",
      "2011-01-11 02:00:00                 0     2    1      1  2011    False   \n",
      "2011-01-11 03:00:00                 0     3    1      1  2011    False   \n",
      "\n",
      "                     lag_1h  lag_24h  lag_1_week  avg_disruptions_24h  \n",
      "start_time                                                             \n",
      "2011-01-10 23:00:00     0.0      0.0         1.0             0.208333  \n",
      "2011-01-11 00:00:00     0.0      0.0         1.0             0.208333  \n",
      "2011-01-11 01:00:00     0.0      0.0         0.0             0.208333  \n",
      "2011-01-11 02:00:00     0.0      0.0         0.0             0.208333  \n",
      "2011-01-11 03:00:00     0.0      0.0         0.0             0.208333  \n",
      "RMSE is: 1.0003992041739744\n",
      "count    15729.000000\n",
      "mean         0.620383\n",
      "std          1.031680\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max         24.000000\n",
      "Name: disruption_count, dtype: float64\n",
      "Accuracy: 0.626676838959883\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "No Disruption (0)       0.79      0.54      0.64      9775\n",
      "   Disruption (1)       0.50      0.76      0.61      5954\n",
      "\n",
      "         accuracy                           0.63     15729\n",
      "        macro avg       0.65      0.65      0.63     15729\n",
      "     weighted avg       0.68      0.63      0.63     15729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disruptions_folder = 'C:/Users/frank/Downloads/TIL Programming/Disruptions'\n",
    "\n",
    "def process_data(folder):\n",
    "    csv_files = glob.glob(os.path.join(disruptions_folder, \"disruptions-*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV in '{disruptions_folder}'.\")\n",
    "        return None\n",
    "\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    disruptions_df = pd.concat(df_list, ignore_index=True)\n",
    "    disruptions_df['start_time'] = pd.to_datetime(disruptions_df['start_time'])\n",
    "    disruptions_df['end_time'] = pd.to_datetime(disruptions_df['end_time'])\n",
    "    \n",
    "    Q1 = disruptions_df['duration_minutes'].quantile(0.25)\n",
    "    Q3 = disruptions_df['duration_minutes'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR    \n",
    "    outliers = disruptions_df[disruptions_df['duration_minutes'] > upper_bound] \n",
    "    \n",
    "    df = disruptions_df[disruptions_df['duration_minutes'] <= upper_bound].copy()\n",
    "    return df\n",
    "\n",
    "total_disruption_df = process_data(disruptions_folder)\n",
    "if total_disruption_df is not None:\n",
    "    df_indexed = total_disruption_df.set_index('start_time')\n",
    "    df_per_hour = df_indexed.resample('h').size().to_frame('disruption_count')\n",
    "    #print(df_per_hour)\n",
    "    df_per_hour['hour'] = df_per_hour.index.hour\n",
    "    df_per_hour['day'] = df_per_hour.index.dayofweek\n",
    "    df_per_hour['month'] = df_per_hour.index.month\n",
    "    df_per_hour['year'] = df_per_hour.index.year \n",
    "    #print(df_per_hour)\n",
    "    is_weekday = df_per_hour['day'] < 5\n",
    "    is_morning_peak = (df_per_hour['hour'] >= 6) & (df_per_hour['hour'] < 9)\n",
    "    is_evening_peak = (df_per_hour['hour'] >= 16) & (df_per_hour['hour'] < 19)\n",
    "    df_per_hour['is_peak'] = (is_weekday & (is_morning_peak | is_evening_peak))    \n",
    "    #print(df_per_hour)\n",
    "    #Create lagged feature, https://scikit-learn.org/stable/auto_examples/applications/plot_time_series_lagged_features.html    \n",
    "    df_per_hour['lag_1h'] = df_per_hour['disruption_count'].shift(1)\n",
    "    df_per_hour['lag_24h'] = df_per_hour['disruption_count'].shift(24)\n",
    "    df_per_hour['lag_1_week'] = df_per_hour['disruption_count'].shift(24 * 7)\n",
    "    df_per_hour['avg_disruptions_24h'] = df_per_hour['disruption_count'].shift(1).rolling(window=24).mean()\n",
    "    df_per_hour = df_per_hour.dropna()\n",
    "    print(df_per_hour.head())  \n",
    "    target = 'disruption_count'\n",
    "    feature_columns = [column for column in df_per_hour.columns if column != target]\n",
    "    \n",
    "    X = df_per_hour[feature_columns]\n",
    "    Y = df_per_hour[target]    \n",
    "\n",
    "    test_percentage = 0.2\n",
    "    split = int(len(df_per_hour) * (1-test_percentage))\n",
    "\n",
    "    X_train = X.iloc[:split]\n",
    "    Y_train = Y.iloc[:split]\n",
    "    X_test = X.iloc[split:]\n",
    "    Y_test = Y.iloc[split:]\n",
    "    Y_test_binary = (Y_test > 0).astype(int)\n",
    "\n",
    "    random_forest_model = RandomForestRegressor(n_estimators=100, random_state = 42, n_jobs = -1)\n",
    "    random_forest_model.fit(X_train, Y_train)\n",
    "    predict = random_forest_model.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(Y_test, predict))\n",
    "    print(f\"RMSE is: {RMSE}\")\n",
    "    print(Y_test.describe())\n",
    "    predict_binary = (np.round(predict) > 0).astype(int)\n",
    "    accuracy = accuracy_score(Y_test_binary, predict_binary)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(classification_report(Y_test_binary, predict_binary, target_names=['No Disruption (0)', 'Disruption (1)']))\n",
    "    \n",
    "    #print(f\"RMSE of the random forest regressor is: {RMSE}\")\n",
    "    #print(Y_test.describe())\n",
    "\n",
    "    #print(f\"X_train = {X_train.head(10)}\\n\\nY_train = {Y_train.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66cc13e7-99ed-4214-88d2-3b2bf593d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     disruption_count  hour  day  month  is_peak  lag_1h  \\\n",
      "start_time                                                                 \n",
      "2011-01-10 23:00:00                 0    23    0      1    False     0.0   \n",
      "2011-01-11 00:00:00                 0     0    1      1    False     0.0   \n",
      "2011-01-11 01:00:00                 0     1    1      1    False     0.0   \n",
      "2011-01-11 02:00:00                 0     2    1      1    False     0.0   \n",
      "2011-01-11 03:00:00                 0     3    1      1    False     0.0   \n",
      "\n",
      "                     lag_24h  lag_1_week  avg_disruptions_24h  \n",
      "start_time                                                     \n",
      "2011-01-10 23:00:00      0.0         1.0             0.208333  \n",
      "2011-01-11 00:00:00      0.0         1.0             0.208333  \n",
      "2011-01-11 01:00:00      0.0         0.0             0.208333  \n",
      "2011-01-11 02:00:00      0.0         0.0             0.208333  \n",
      "2011-01-11 03:00:00      0.0         0.0             0.208333  \n",
      "disruption_count\n",
      "0    48976\n",
      "1    13939\n",
      "Name: count, dtype: int64\n",
      "0.6261682242990654\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "No Disruption (0)       0.65      0.87      0.74      9775\n",
      "   Disruption (1)       0.51      0.23      0.32      5954\n",
      "\n",
      "         accuracy                           0.63     15729\n",
      "        macro avg       0.58      0.55      0.53     15729\n",
      "     weighted avg       0.60      0.63      0.58     15729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disruptions_folder = 'C:/Users/frank/Downloads/TIL Programming/Disruptions'\n",
    "\n",
    "def process_data(folder):\n",
    "    csv_files = glob.glob(os.path.join(disruptions_folder, \"disruptions-*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV in '{disruptions_folder}'.\")\n",
    "        return None\n",
    "\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    disruptions_df = pd.concat(df_list, ignore_index=True)\n",
    "    disruptions_df['start_time'] = pd.to_datetime(disruptions_df['start_time'])\n",
    "    disruptions_df['end_time'] = pd.to_datetime(disruptions_df['end_time'])\n",
    "    \n",
    "    Q1 = disruptions_df['duration_minutes'].quantile(0.25)\n",
    "    Q3 = disruptions_df['duration_minutes'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR    \n",
    "    outliers = disruptions_df[disruptions_df['duration_minutes'] > upper_bound] \n",
    "    \n",
    "    df = disruptions_df[disruptions_df['duration_minutes'] <= upper_bound].copy()\n",
    "    return df\n",
    "\n",
    "total_disruption_df = process_data(disruptions_folder)\n",
    "if total_disruption_df is not None:\n",
    "    df_indexed = total_disruption_df.set_index('start_time')\n",
    "    df_per_hour = df_indexed.resample('h').size().to_frame('disruption_count')\n",
    "    #print(df_per_hour)\n",
    "    df_per_hour['hour'] = df_per_hour.index.hour\n",
    "    df_per_hour['day'] = df_per_hour.index.dayofweek\n",
    "    df_per_hour['month'] = df_per_hour.index.month\n",
    "    #df_per_hour['year'] = df_per_hour.index.year \n",
    "    #print(df_per_hour)\n",
    "    is_weekday = df_per_hour['day'] < 5\n",
    "    is_morning_peak = (df_per_hour['hour'] >= 6) & (df_per_hour['hour'] < 9)\n",
    "    is_evening_peak = (df_per_hour['hour'] >= 16) & (df_per_hour['hour'] < 19)\n",
    "    df_per_hour['is_peak'] = (is_weekday & (is_morning_peak | is_evening_peak))    \n",
    "    #print(df_per_hour)\n",
    "    #Create lagged feature, https://scikit-learn.org/stable/auto_examples/applications/plot_time_series_lagged_features.html    \n",
    "    df_per_hour['lag_1h'] = df_per_hour['disruption_count'].shift(1)\n",
    "    df_per_hour['lag_24h'] = df_per_hour['disruption_count'].shift(24)\n",
    "    df_per_hour['lag_1_week'] = df_per_hour['disruption_count'].shift(24 * 7)\n",
    "    df_per_hour['avg_disruptions_24h'] = df_per_hour['disruption_count'].shift(1).rolling(window=24).mean()\n",
    "    df_per_hour = df_per_hour.dropna()\n",
    "    print(df_per_hour.head())  \n",
    "    target = 'disruption_count'\n",
    "    feature_columns = [column for column in df_per_hour.columns if column != target]\n",
    "    \n",
    "    X = df_per_hour[feature_columns]\n",
    "    Y = df_per_hour[target]    \n",
    "\n",
    "    test_percentage = 0.2\n",
    "    split = int(len(df_per_hour) * (1-test_percentage))\n",
    "\n",
    "    X_train = X.iloc[:split]\n",
    "    Y_train = (Y.iloc[:split] > 0).astype(int)\n",
    "    X_test = X.iloc[split:]\n",
    "    Y_test = (Y.iloc[split:] > 0).astype(int)\n",
    "\n",
    "    print(Y_train.value_counts())\n",
    "\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=100, random_state = 42, n_jobs = -1, class_weight = 'balanced')    \n",
    "    random_forest_model.fit(X_train, Y_train)\n",
    "    predict = random_forest_model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, predict)\n",
    "    print(accuracy)\n",
    "    print(classification_report(Y_test, predict, target_names=['No Disruption (0)', 'Disruption (1)']))\n",
    "    \n",
    "    #print(f\"RMSE of the random forest regressor is: {RMSE}\")\n",
    "    #print(Y_test.describe())\n",
    "\n",
    "    #print(f\"X_train = {X_train.head(10)}\\n\\nY_train = {Y_train.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88b016-7123-4539-bc7a-8e754a1a4753",
   "metadata": {},
   "source": [
    "Now, we will implement using the statsmodel module, to try and see if better results can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d51bd5c-28ae-4da1-9d88-5529245fb7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     disruption_count  hour  day  month  is_peak  lag_1h  \\\n",
      "start_time                                                                 \n",
      "2011-01-10 23:00:00                 0    23    0      1    False     0.0   \n",
      "2011-01-11 00:00:00                 0     0    1      1    False     0.0   \n",
      "2011-01-11 01:00:00                 0     1    1      1    False     0.0   \n",
      "2011-01-11 02:00:00                 0     2    1      1    False     0.0   \n",
      "2011-01-11 03:00:00                 0     3    1      1    False     0.0   \n",
      "\n",
      "                     lag_24h  lag_1_week  avg_disruptions_24h  \n",
      "start_time                                                     \n",
      "2011-01-10 23:00:00      0.0         1.0             0.208333  \n",
      "2011-01-11 00:00:00      0.0         1.0             0.208333  \n",
      "2011-01-11 01:00:00      0.0         0.0             0.208333  \n",
      "2011-01-11 02:00:00      0.0         0.0             0.208333  \n",
      "2011-01-11 03:00:00      0.0         0.0             0.208333  \n",
      "RMSE is: 3.418376029435485\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "No Disruption (0)       0.71      0.75      0.73      9775\n",
      "   Disruption (1)       0.54      0.50      0.52      5954\n",
      "\n",
      "         accuracy                           0.65     15729\n",
      "        macro avg       0.63      0.62      0.62     15729\n",
      "     weighted avg       0.65      0.65      0.65     15729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disruptions_folder = 'C:/Users/frank/Downloads/TIL Programming/Disruptions'\n",
    "\n",
    "def process_data(folder):\n",
    "    csv_files = glob.glob(os.path.join(disruptions_folder, \"disruptions-*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV in '{disruptions_folder}'.\")\n",
    "        return None\n",
    "\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    disruptions_df = pd.concat(df_list, ignore_index=True)\n",
    "    disruptions_df['start_time'] = pd.to_datetime(disruptions_df['start_time'])\n",
    "    disruptions_df['end_time'] = pd.to_datetime(disruptions_df['end_time'])\n",
    "    \n",
    "    Q1 = disruptions_df['duration_minutes'].quantile(0.25)\n",
    "    Q3 = disruptions_df['duration_minutes'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR    \n",
    "    outliers = disruptions_df[disruptions_df['duration_minutes'] > upper_bound] \n",
    "    \n",
    "    df = disruptions_df[disruptions_df['duration_minutes'] <= upper_bound].copy()\n",
    "    return df\n",
    "\n",
    "total_disruption_df = process_data(disruptions_folder)\n",
    "if total_disruption_df is not None:\n",
    "    df_indexed = total_disruption_df.set_index('start_time')\n",
    "    df_per_hour = df_indexed.resample('h').size().to_frame('disruption_count')\n",
    "    #print(df_per_hour)\n",
    "    df_per_hour['hour'] = df_per_hour.index.hour\n",
    "    df_per_hour['day'] = df_per_hour.index.dayofweek\n",
    "    df_per_hour['month'] = df_per_hour.index.month\n",
    "    #df_per_hour['year'] = df_per_hour.index.year \n",
    "    #print(df_per_hour)\n",
    "    is_weekday = df_per_hour['day'] < 5\n",
    "    is_morning_peak = (df_per_hour['hour'] >= 6) & (df_per_hour['hour'] < 9)\n",
    "    is_evening_peak = (df_per_hour['hour'] >= 16) & (df_per_hour['hour'] < 19)\n",
    "    df_per_hour['is_peak'] = (is_weekday & (is_morning_peak | is_evening_peak))    \n",
    "    #print(df_per_hour)\n",
    "    #Create lagged feature, https://scikit-learn.org/stable/auto_examples/applications/plot_time_series_lagged_features.html    \n",
    "    df_per_hour['lag_1h'] = df_per_hour['disruption_count'].shift(1)\n",
    "    df_per_hour['lag_24h'] = df_per_hour['disruption_count'].shift(24)\n",
    "    df_per_hour['lag_1_week'] = df_per_hour['disruption_count'].shift(24 * 7)\n",
    "    df_per_hour['avg_disruptions_24h'] = df_per_hour['disruption_count'].shift(1).rolling(window=24).mean()\n",
    "    df_per_hour = df_per_hour.dropna()\n",
    "    print(df_per_hour.head())  \n",
    "    target = 'disruption_count'\n",
    "    feature_columns = [column for column in df_per_hour.columns if column != target]\n",
    "    X = df_per_hour[feature_columns]\n",
    "    Y = df_per_hour[target]    \n",
    "\n",
    "    test_percentage = 0.2\n",
    "    split = int(len(df_per_hour) * (1-test_percentage))\n",
    "\n",
    "    X_train = X.iloc[:split].astype(float)\n",
    "    Y_train = Y.iloc[:split]\n",
    "    X_test = X.iloc[split:].astype(float)\n",
    "    Y_test = Y.iloc[split:]    \n",
    "\n",
    "    X_train_const = sm.add_constant(X_train, prepend=False)\n",
    "    X_test_const = sm.add_constant(X_test, prepend=False)\n",
    "\n",
    "    model = sm.ZeroInflatedNegativeBinomialP(endog=Y_train, exog=X_train_const)\n",
    "    results = model.fit(method='bfgs', maxiter=500, disp=False)\n",
    "    predict = np.round(results.predict(X_test_const)).astype(int)\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(Y_test, predict))\n",
    "    print(f\"RMSE is: {RMSE}\")\n",
    "\n",
    "    predict_binary = (predict > 0).astype(int)\n",
    "    Y_test_binary = (Y_test > 0).astype(int)\n",
    "    print(classification_report(Y_test_binary, predict_binary, target_names=['No Disruption (0)', 'Disruption (1)']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
