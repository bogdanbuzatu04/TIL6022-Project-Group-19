{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5284ab1-6730-473b-a86d-727bb85e726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84da2c1-a162-4224-8505-cf0a06393696",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we will analyze all of the data in the Disruptions folder. This will be done in the codeblock below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cc13e7-99ed-4214-88d2-3b2bf593d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     disruption_count  hour  day  month  year  is_peak  \\\n",
      "start_time                                                               \n",
      "2011-01-10 23:00:00                 0    23    0      1  2011    False   \n",
      "2011-01-11 00:00:00                 0     0    1      1  2011    False   \n",
      "2011-01-11 01:00:00                 0     1    1      1  2011    False   \n",
      "2011-01-11 02:00:00                 0     2    1      1  2011    False   \n",
      "2011-01-11 03:00:00                 0     3    1      1  2011    False   \n",
      "...                               ...   ...  ...    ...   ...      ...   \n",
      "2019-12-31 14:00:00                 0    14    1     12  2019    False   \n",
      "2019-12-31 15:00:00                 0    15    1     12  2019    False   \n",
      "2019-12-31 16:00:00                 0    16    1     12  2019     True   \n",
      "2019-12-31 17:00:00                 0    17    1     12  2019     True   \n",
      "2019-12-31 18:00:00                 1    18    1     12  2019     True   \n",
      "\n",
      "                     lag_1h  lag_24h  lag_1_week  avg_disruptions_24h  \n",
      "start_time                                                             \n",
      "2011-01-10 23:00:00     0.0      0.0         1.0             0.208333  \n",
      "2011-01-11 00:00:00     0.0      0.0         1.0             0.208333  \n",
      "2011-01-11 01:00:00     0.0      0.0         0.0             0.208333  \n",
      "2011-01-11 02:00:00     0.0      0.0         0.0             0.208333  \n",
      "2011-01-11 03:00:00     0.0      0.0         0.0             0.208333  \n",
      "...                     ...      ...         ...                  ...  \n",
      "2019-12-31 14:00:00     0.0      2.0         0.0             0.625000  \n",
      "2019-12-31 15:00:00     0.0      1.0         0.0             0.541667  \n",
      "2019-12-31 16:00:00     0.0      2.0         3.0             0.500000  \n",
      "2019-12-31 17:00:00     0.0      1.0         0.0             0.416667  \n",
      "2019-12-31 18:00:00     0.0      1.0         0.0             0.375000  \n",
      "\n",
      "[78644 rows x 10 columns]\n",
      "2.45212277364292\n",
      "count    15729.000000\n",
      "mean         0.620383\n",
      "std          1.031680\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          1.000000\n",
      "max         24.000000\n",
      "Name: disruption_count, dtype: float64\n",
      "X_train =                      hour  day  month  year  is_peak  lag_1h  lag_24h  \\\n",
      "start_time                                                              \n",
      "2011-01-10 23:00:00    23    0      1  2011    False     0.0      0.0   \n",
      "2011-01-11 00:00:00     0    1      1  2011    False     0.0      0.0   \n",
      "2011-01-11 01:00:00     1    1      1  2011    False     0.0      0.0   \n",
      "2011-01-11 02:00:00     2    1      1  2011    False     0.0      0.0   \n",
      "2011-01-11 03:00:00     3    1      1  2011    False     0.0      0.0   \n",
      "2011-01-11 04:00:00     4    1      1  2011    False     0.0      0.0   \n",
      "2011-01-11 05:00:00     5    1      1  2011    False     0.0      0.0   \n",
      "2011-01-11 06:00:00     6    1      1  2011     True     0.0      2.0   \n",
      "2011-01-11 07:00:00     7    1      1  2011     True     1.0      1.0   \n",
      "2011-01-11 08:00:00     8    1      1  2011     True     0.0      0.0   \n",
      "\n",
      "                     lag_1_week  avg_disruptions_24h  \n",
      "start_time                                            \n",
      "2011-01-10 23:00:00         1.0             0.208333  \n",
      "2011-01-11 00:00:00         1.0             0.208333  \n",
      "2011-01-11 01:00:00         0.0             0.208333  \n",
      "2011-01-11 02:00:00         0.0             0.208333  \n",
      "2011-01-11 03:00:00         0.0             0.208333  \n",
      "2011-01-11 04:00:00         0.0             0.208333  \n",
      "2011-01-11 05:00:00         0.0             0.208333  \n",
      "2011-01-11 06:00:00         1.0             0.208333  \n",
      "2011-01-11 07:00:00         2.0             0.166667  \n",
      "2011-01-11 08:00:00         0.0             0.125000  \n",
      "\n",
      "Y_train = start_time\n",
      "2011-01-10 23:00:00    0\n",
      "2011-01-11 00:00:00    0\n",
      "2011-01-11 01:00:00    0\n",
      "2011-01-11 02:00:00    0\n",
      "2011-01-11 03:00:00    0\n",
      "2011-01-11 04:00:00    0\n",
      "2011-01-11 05:00:00    0\n",
      "2011-01-11 06:00:00    1\n",
      "2011-01-11 07:00:00    0\n",
      "2011-01-11 08:00:00    2\n",
      "Freq: h, Name: disruption_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "disruptions_folder = 'C:/Users/frank/Downloads/TIL Programming/Disruptions'\n",
    "\n",
    "def process_data(folder):\n",
    "    csv_files = glob.glob(os.path.join(disruptions_folder, \"disruptions-*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV in '{disruptions_folder}'.\")\n",
    "        return None\n",
    "\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    disruptions_df = pd.concat(df_list, ignore_index=True)\n",
    "    disruptions_df['start_time'] = pd.to_datetime(disruptions_df['start_time'])\n",
    "    disruptions_df['end_time'] = pd.to_datetime(disruptions_df['end_time'])\n",
    "    \n",
    "    Q1 = disruptions_df['duration_minutes'].quantile(0.25)\n",
    "    Q3 = disruptions_df['duration_minutes'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR    \n",
    "    outliers = disruptions_df[disruptions_df['duration_minutes'] > upper_bound] \n",
    "    \n",
    "    df = disruptions_df[disruptions_df['duration_minutes'] <= upper_bound].copy()\n",
    "    return df\n",
    "\n",
    "total_disruption_df = process_data(disruptions_folder)\n",
    "if total_disruption_df is not None:\n",
    "    df_per_hour = total_disruption_df.set_index('start_time').resample('h').size().to_frame('disruption_count')    \n",
    "    #print(df_per_hour)\n",
    "    df_per_hour['hour'] = df_per_hour.index.hour\n",
    "    df_per_hour['day'] = df_per_hour.index.dayofweek\n",
    "    df_per_hour['month'] = df_per_hour.index.month\n",
    "    df_per_hour['year'] = df_per_hour.index.year    \n",
    "    #print(df_per_hour)\n",
    "    is_weekday = df_per_hour['day'] < 5\n",
    "    is_morning_peak = (df_per_hour['hour'] >= 6) & (df_per_hour['hour'] < 9)\n",
    "    is_evening_peak = (df_per_hour['hour'] >= 16) & (df_per_hour['hour'] < 19)\n",
    "    df_per_hour['is_peak'] = (is_weekday & (is_morning_peak | is_evening_peak))    \n",
    "    #print(df_per_hour)\n",
    "    #Create lagged feature, https://scikit-learn.org/stable/auto_examples/applications/plot_time_series_lagged_features.html\n",
    "    df_per_hour['lag_1h'] = df_per_hour['disruption_count'].shift(1)\n",
    "    df_per_hour['lag_24h'] = df_per_hour['disruption_count'].shift(24)\n",
    "    df_per_hour['lag_1_week'] = df_per_hour['disruption_count'].shift(24 * 7)\n",
    "    df_per_hour['avg_disruptions_24h'] = df_per_hour['disruption_count'].shift(1).rolling(window=24).mean()\n",
    "    df_per_hour = df_per_hour.dropna()\n",
    "    print(df_per_hour)    \n",
    "    target = 'disruption_count'\n",
    "    feature_columns = [column for column in df_per_hour.columns if column != target]\n",
    "    \n",
    "    X = df_per_hour[feature_columns]\n",
    "    Y = df_per_hour[target]    \n",
    "\n",
    "    test_percentage = 0.2\n",
    "    split = int(len(df_per_hour) * (1-test_percentage))\n",
    "\n",
    "    X_train = X.iloc[:split]\n",
    "    Y_train = Y.iloc[:split]\n",
    "    X_test = X.iloc[split:]\n",
    "    Y_test = Y.iloc[split:]\n",
    "\n",
    "    poisson_model = PoissonRegressor(alpha = 1e-3, max_iter = 500)    \n",
    "    poisson_model.fit(X_train, Y_train)\n",
    "    predict = poisson_model.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    RMSE = np.sqrt(mean_squared_error(Y_test, predict))\n",
    "    print(RMSE)\n",
    "    print(Y_test.describe())\n",
    "\n",
    "    print(f\"X_train = {X_train.head(10)}\\n\\nY_train = {Y_train.head(10)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
